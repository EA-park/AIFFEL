{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLlh3ibhJHxnRp9brV8tFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EA-park/AIFFEL/blob/main/%5BExp_06%5DLyrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#루브릭\n",
        "\n",
        "1. 데이터의 전처리 및 구성과정이 체계적으로 진행되었는가?\n",
        " - 특수문자 제거, 토크나이저 생성, 패딩 처리의 작업들이 빠짐없이 진행되었는가?\n",
        "\n",
        "2. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
        " - 텍스트 제너레이션 결과로 생성된 문장이 해석 가능한 문장인가?\n",
        "\n",
        "3. 텍스트 생성모델이 안정적으로 학습되었는가?\n",
        " - 텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?"
      ],
      "metadata": {
        "id": "gFoSegu9CnJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리"
      ],
      "metadata": {
        "id": "PIgNYGC_C-7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P0IaaLS-_Qhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b777ea9-7fac-46f2-e34d-4804ed828ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "# set the working directory\n",
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "# read files\n",
        "import glob\n",
        "\n",
        "# model\n",
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. 경로 설정"
      ],
      "metadata": {
        "id": "Lfb56vf1DCyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = '/content/gdrive'\n",
        "drive.mount(ROOT)"
      ],
      "metadata": {
        "id": "1yWMQ_onDDRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee555b60-b7bf-4033-cf02-d00e3ac03c27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJ = 'MyDrive/AIFFEL/Exp-06'\n",
        "WORKING_DIR = join(ROOT, PROJ)\n",
        "# train data\n",
        "lyrics_path = join(WORKING_DIR, 'lyrics')"
      ],
      "metadata": {
        "id": "k3jEJYCoiP3i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. 데이터 읽어오기"
      ],
      "metadata": {
        "id": "fbm3MtueDEdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_list = glob.glob(lyrics_path + \"/*.txt\")\n",
        "raw_corpus = [] \n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
        "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
        "\n",
        "print(f\"데이터 크기: {len(raw_corpus):,}\")\n",
        "print(f\"Examples:\\n{raw_corpus[:3]}\")"
      ],
      "metadata": {
        "id": "ERj17-FFDEt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50271eb-4d89-4029-a3bd-75e88dcb1060"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187,088\n",
            "Examples:\n",
            "['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. 데이터 정제"
      ],
      "metadata": {
        "id": "Uh6ndMj5DGIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "metadata": {
        "id": "_nmsDwFSDGWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b6ee6d-1171-4f6d-b31e-2189ad2a95b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. 평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "IzUojodUDHep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQhSa_GtlnV2",
        "outputId": "f62dac6f-db72-4564-dd8a-203515c3b97a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
        "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "def tokenize(corpus):\n",
        "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
        "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
        "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Wwxr_kldNZ",
        "outputId": "2c3927db-6c3c-446f-cb38-375519b0aaa8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 304  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2  23  77 ...   0   0   0]\n",
            " [  2  42  26 ...   0   0   0]\n",
            " [  2  23  77 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f0c72aeb490>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]  \n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "id": "2jRwz1VnDHsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32788bc6-7dfb-4026-dfa4-fad9fe0f5255"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        " # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
        " # tokenize() 함수에서 num_words를 7000개로 선언했기 때문에, tokenizer.num_words의 값은 7000\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
        "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcrwLAzwm59Z",
        "outputId": "74722755-d717-4b51-ad24-7f6a1b600ed4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 346), dtype=tf.int32, name=None), TensorSpec(shape=(256, 346), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5. 인공지능 만들기"
      ],
      "metadata": {
        "id": "EJPKeoyHDJaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
        "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
        "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
        "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져옵니다!   \n",
        "embedding_size = 256 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기입니다.\n",
        "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
      ],
      "metadata": {
        "id": "40v-XZmMDKEW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUCHWcCZnFrJ",
        "outputId": "fbde2c7e-8f85-44c3-8495-332754400d8f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 346, 7001), dtype=float32, numpy=\n",
              "array([[[-2.24871401e-05,  1.68394909e-04,  9.78832250e-05, ...,\n",
              "         -1.49252301e-04,  1.77002323e-04,  1.19083590e-04],\n",
              "        [ 3.88845103e-04,  3.25279165e-04,  7.36291113e-05, ...,\n",
              "         -1.35124210e-04, -2.58307700e-05,  2.11990104e-04],\n",
              "        [ 8.14394676e-04,  5.02410461e-04,  2.37546279e-04, ...,\n",
              "         -2.39442539e-04,  1.58454117e-04,  1.67243707e-06],\n",
              "        ...,\n",
              "        [ 1.78961200e-04,  1.95842236e-03,  1.48310582e-03, ...,\n",
              "         -1.77435973e-03,  2.07208004e-03,  1.24674669e-04],\n",
              "        [ 1.78960967e-04,  1.95842283e-03,  1.48310559e-03, ...,\n",
              "         -1.77435996e-03,  2.07208004e-03,  1.24674436e-04],\n",
              "        [ 1.78960734e-04,  1.95842329e-03,  1.48310559e-03, ...,\n",
              "         -1.77435996e-03,  2.07208050e-03,  1.24674523e-04]],\n",
              "\n",
              "       [[-2.24871401e-05,  1.68394909e-04,  9.78832250e-05, ...,\n",
              "         -1.49252301e-04,  1.77002323e-04,  1.19083590e-04],\n",
              "        [ 1.00999285e-04,  2.47252348e-04,  5.46047522e-05, ...,\n",
              "         -1.61879434e-04,  3.34502867e-04,  2.43709568e-04],\n",
              "        [ 3.45308275e-04,  3.16130987e-04,  8.14439554e-05, ...,\n",
              "         -2.35167186e-04,  8.15980675e-05,  2.42328926e-04],\n",
              "        ...,\n",
              "        [ 1.78960618e-04,  1.95842143e-03,  1.48310442e-03, ...,\n",
              "         -1.77435949e-03,  2.07208213e-03,  1.24674640e-04],\n",
              "        [ 1.78960618e-04,  1.95842166e-03,  1.48310512e-03, ...,\n",
              "         -1.77435926e-03,  2.07208144e-03,  1.24674538e-04],\n",
              "        [ 1.78960385e-04,  1.95842190e-03,  1.48310512e-03, ...,\n",
              "         -1.77435949e-03,  2.07208190e-03,  1.24674523e-04]],\n",
              "\n",
              "       [[-2.24871401e-05,  1.68394909e-04,  9.78832250e-05, ...,\n",
              "         -1.49252301e-04,  1.77002323e-04,  1.19083590e-04],\n",
              "        [ 5.35467989e-04,  3.31545772e-04,  2.63394089e-04, ...,\n",
              "         -1.71669788e-04,  3.34583950e-04,  1.51022221e-04],\n",
              "        [ 7.60952476e-04,  3.77892895e-04,  2.86810071e-04, ...,\n",
              "         -1.64187979e-04,  1.13000046e-04,  1.12191716e-04],\n",
              "        ...,\n",
              "        [ 1.78960036e-04,  1.95842329e-03,  1.48310629e-03, ...,\n",
              "         -1.77436159e-03,  2.07208237e-03,  1.24674683e-04],\n",
              "        [ 1.78960036e-04,  1.95842329e-03,  1.48310652e-03, ...,\n",
              "         -1.77436136e-03,  2.07208237e-03,  1.24674989e-04],\n",
              "        [ 1.78959803e-04,  1.95842353e-03,  1.48310582e-03, ...,\n",
              "         -1.77436159e-03,  2.07208213e-03,  1.24674945e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-2.24871401e-05,  1.68394909e-04,  9.78832250e-05, ...,\n",
              "         -1.49252301e-04,  1.77002323e-04,  1.19083590e-04],\n",
              "        [ 6.66527922e-05, -5.61593042e-05,  3.72229289e-04, ...,\n",
              "         -1.70619373e-04,  2.21150563e-04,  3.00348765e-05],\n",
              "        [ 4.53029788e-04, -3.72363400e-04,  6.44125277e-04, ...,\n",
              "          2.06225232e-04,  4.59145114e-04, -3.18612671e-04],\n",
              "        ...,\n",
              "        [ 1.78960501e-04,  1.95842516e-03,  1.48310536e-03, ...,\n",
              "         -1.77435996e-03,  2.07208190e-03,  1.24674945e-04],\n",
              "        [ 1.78960734e-04,  1.95842516e-03,  1.48310512e-03, ...,\n",
              "         -1.77435984e-03,  2.07208190e-03,  1.24674872e-04],\n",
              "        [ 1.78960734e-04,  1.95842562e-03,  1.48310605e-03, ...,\n",
              "         -1.77436019e-03,  2.07208237e-03,  1.24674509e-04]],\n",
              "\n",
              "       [[-2.24871401e-05,  1.68394909e-04,  9.78832250e-05, ...,\n",
              "         -1.49252301e-04,  1.77002323e-04,  1.19083590e-04],\n",
              "        [ 5.35467989e-04,  3.31545772e-04,  2.63394089e-04, ...,\n",
              "         -1.71669788e-04,  3.34583950e-04,  1.51022221e-04],\n",
              "        [ 6.73057337e-04,  4.96583874e-04,  3.68860172e-04, ...,\n",
              "         -2.48735683e-04,  5.65903843e-04,  2.03332456e-04],\n",
              "        ...,\n",
              "        [ 1.78960734e-04,  1.95842260e-03,  1.48310536e-03, ...,\n",
              "         -1.77435996e-03,  2.07208213e-03,  1.24674902e-04],\n",
              "        [ 1.78960152e-04,  1.95842166e-03,  1.48310536e-03, ...,\n",
              "         -1.77435973e-03,  2.07208190e-03,  1.24674785e-04],\n",
              "        [ 1.78960501e-04,  1.95842213e-03,  1.48310512e-03, ...,\n",
              "         -1.77435996e-03,  2.07208213e-03,  1.24674552e-04]],\n",
              "\n",
              "       [[-2.24871401e-05,  1.68394909e-04,  9.78832250e-05, ...,\n",
              "         -1.49252301e-04,  1.77002323e-04,  1.19083590e-04],\n",
              "        [ 3.88845103e-04,  3.25279165e-04,  7.36291113e-05, ...,\n",
              "         -1.35124210e-04, -2.58307700e-05,  2.11990104e-04],\n",
              "        [ 8.73251003e-04,  1.09609653e-04,  4.83780168e-05, ...,\n",
              "         -4.87051730e-05, -1.20434561e-05,  2.83634872e-04],\n",
              "        ...,\n",
              "        [ 1.78960967e-04,  1.95842376e-03,  1.48310629e-03, ...,\n",
              "         -1.77436206e-03,  2.07208097e-03,  1.24674407e-04],\n",
              "        [ 1.78961200e-04,  1.95842353e-03,  1.48310652e-03, ...,\n",
              "         -1.77436194e-03,  2.07208097e-03,  1.24674392e-04],\n",
              "        [ 1.78960967e-04,  1.95842376e-03,  1.48310652e-03, ...,\n",
              "         -1.77436171e-03,  2.07208144e-03,  1.24674625e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 구조를 확인합니다.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C3kyei5nH7I",
        "outputId": "5d19105b-b208-43bb-b492-8a803fe512f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  1792256   \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  7176025   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,607,961\n",
            "Trainable params: 22,607,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer와 loss등은 차차 배웁니다\n",
        "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
        "\n",
        "# Adam 알고리즘을 구현하는 optimzier이며 어떤 optimzier를 써야할지 모른다면 Adam을 쓰는 것도 방법이다.\n",
        "# 우리가 학습을 할 때 최대한 틀리지 않는 방향으로 학습을 해야한다.\n",
        "# 여기서 얼마나 틀리는지(loss)를 알게하는 함수가 손실함수 이다.\n",
        "# 이 손실함수의 최소값을 찾는 것을 학습의 목표로 하며 여기서 최소값을 찾아가는 과정을 optimization 이라하고\n",
        "# 이를 수행하는 알고리즘을 optimizer(최적화)라고 한다.\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저이다. 자세한 내용은 차차 배운다.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수이다.\n",
        "    from_logits=True, # 기본값은 False이다. 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려준다. 즉 softmax함수가 적용되지 않았다는걸 의미한다. \n",
        "    reduction='none'  # 기본값은 SUM이다. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",
        ")\n",
        "# 모델을 학습시키키 위한 학습과정을 설정하는 단계이다.\n",
        "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정했다.\n",
        "model.fit(dataset, epochs=30) # 만들어둔 데이터셋으로 모델을 학습한다. 30번 학습을 반복하겠다는 의미다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhFb5y3fnLSD",
        "outputId": "8bd7a847-7c26-4672-854b-148a0d0b9092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            " 56/686 [=>............................] - ETA: 21:24:23 - loss: 0.6604"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6. 평가하기"
      ],
      "metadata": {
        "id": "tXpBa3WtDPcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#문장생성 함수 정의\n",
        "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
        "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4 \n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated #최종적으로 모델이 생성한 문장을 반환"
      ],
      "metadata": {
        "id": "AYJcC8ERDR2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "id": "Lh5tlaNinVGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#회고"
      ],
      "metadata": {
        "id": "lCQjAJfHFex2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 이번 프로젝트에서 어려웠던 점\n",
        "\n",
        "2. 프로젝트를 진행하면서 알아낸 점 혹은 아직 모호한 점\n",
        "\n",
        " 1) 알아낸 점\n",
        "\n",
        " 2) 모호한 점\n",
        "\n",
        "3. 루브릭 평가 지표를 맞추기 위해 시도한 것들\n",
        "\n",
        "4. 다짐"
      ],
      "metadata": {
        "id": "V-DOzLWaxwDG"
      }
    }
  ]
}